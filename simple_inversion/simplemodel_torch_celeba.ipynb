{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1acc879",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92ab3f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "ename": "FileURLRetrievalError",
     "evalue": "Failed to retrieve file url:\n\n\tToo many users have viewed or downloaded this file recently. Please\n\ttry accessing the file again later. If the file you are trying to\n\taccess is particularly large or is shared with many people, it may\n\ttake up to 24 hours to be able to view or download the file. If you\n\tstill can't access a file after 24 hours, contact your domain\n\tadministrator.\n\nYou may still be able to access the file from the browser:\n\n\thttps://drive.google.com/uc?id=0B7EVK8r0v71pZjFTYXZWM3FlRnM\n\nbut Gdown can't. Please check connections and permissions.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileURLRetrievalError\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dones\\Documents\\model-inversion\\.venv\\Lib\\site-packages\\gdown\\download.py:267\u001b[39m, in \u001b[36mdownload\u001b[39m\u001b[34m(url, output, quiet, proxy, speed, use_cookies, verify, id, fuzzy, resume, format, user_agent, log_messages)\u001b[39m\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m     url = \u001b[43mget_url_from_gdrive_confirmation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m FileURLRetrievalError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dones\\Documents\\model-inversion\\.venv\\Lib\\site-packages\\gdown\\download.py:53\u001b[39m, in \u001b[36mget_url_from_gdrive_confirmation\u001b[39m\u001b[34m(contents)\u001b[39m\n\u001b[32m     52\u001b[39m         error = m.groups()[\u001b[32m0\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m FileURLRetrievalError(error)\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m url:\n",
      "\u001b[31mFileURLRetrievalError\u001b[39m: Too many users have viewed or downloaded this file recently. Please try accessing the file again later. If the file you are trying to access is particularly large or is shared with many people, it may take up to 24 hours to be able to view or download the file. If you still can't access a file after 24 hours, contact your domain administrator.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mFileURLRetrievalError\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     20\u001b[39m transform = transforms.Compose([\n\u001b[32m     21\u001b[39m     transforms.Grayscale(num_output_channels=\u001b[32m1\u001b[39m),\n\u001b[32m     22\u001b[39m     transforms.ToTensor(),\n\u001b[32m     23\u001b[39m ])\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Load CelebA (aligned & cropped)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m dataset = \u001b[43mdatasets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCelebA\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./data\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43midentity\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransform\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Create a subset with 10% of the dataset\u001b[39;00m\n\u001b[32m     35\u001b[39m subset_size = \u001b[38;5;28mint\u001b[39m(\u001b[32m0.1\u001b[39m * \u001b[38;5;28mlen\u001b[39m(dataset))  \u001b[38;5;66;03m# 10% of dataset\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dones\\Documents\\model-inversion\\.venv\\Lib\\site-packages\\torchvision\\datasets\\celeba.py:85\u001b[39m, in \u001b[36mCelebA.__init__\u001b[39m\u001b[34m(self, root, split, target_type, transform, target_transform, download)\u001b[39m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mtarget_transform is specified but target_type is empty\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._check_integrity():\n\u001b[32m     88\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dones\\Documents\\model-inversion\\.venv\\Lib\\site-packages\\torchvision\\datasets\\celeba.py:160\u001b[39m, in \u001b[36mCelebA.download\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (file_id, md5, filename) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.file_list:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     \u001b[43mdownload_file_from_google_drive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_folder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m extract_archive(os.path.join(\u001b[38;5;28mself\u001b[39m.root, \u001b[38;5;28mself\u001b[39m.base_folder, \u001b[33m\"\u001b[39m\u001b[33mimg_align_celeba.zip\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dones\\Documents\\model-inversion\\.venv\\Lib\\site-packages\\torchvision\\datasets\\utils.py:206\u001b[39m, in \u001b[36mdownload_file_from_google_drive\u001b[39m\u001b[34m(file_id, root, filename, md5)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_integrity(fpath, md5):\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m \u001b[43mgdown\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mfile_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUSER_AGENT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_integrity(fpath, md5):\n\u001b[32m    209\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mFile not found or corrupted.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dones\\Documents\\model-inversion\\.venv\\Lib\\site-packages\\gdown\\download.py:278\u001b[39m, in \u001b[36mdownload\u001b[39m\u001b[34m(url, output, quiet, proxy, speed, use_cookies, verify, id, fuzzy, resume, format, user_agent, log_messages)\u001b[39m\n\u001b[32m    268\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m FileURLRetrievalError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    269\u001b[39m         message = (\n\u001b[32m    270\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mFailed to retrieve file url:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    271\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou may still be able to access the file from the browser:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    276\u001b[39m             url_origin,\n\u001b[32m    277\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m FileURLRetrievalError(message)\n\u001b[32m    280\u001b[39m filename_from_url = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    281\u001b[39m last_modified_time = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mFileURLRetrievalError\u001b[39m: Failed to retrieve file url:\n\n\tToo many users have viewed or downloaded this file recently. Please\n\ttry accessing the file again later. If the file you are trying to\n\taccess is particularly large or is shared with many people, it may\n\ttake up to 24 hours to be able to view or download the file. If you\n\tstill can't access a file after 24 hours, contact your domain\n\tadministrator.\n\nYou may still be able to access the file from the browser:\n\n\thttps://drive.google.com/uc?id=0B7EVK8r0v71pZjFTYXZWM3FlRnM\n\nbut Gdown can't. Please check connections and permissions."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Constants\n",
    "IMG_X_SIZE = 218\n",
    "IMG_Y_SIZE = 178\n",
    "NUM_CLASSES = 10  # For demo\n",
    "MODEL_CHECKPOINT_PATH = './face_recognition_model.pth'\n",
    "\n",
    "# Transform: grayscale + tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load CelebA (aligned & cropped)\n",
    "dataset = datasets.CelebA(\n",
    "    root='./data',\n",
    "    split='train',\n",
    "    target_type='identity',\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Create a subset with 10% of the dataset\n",
    "subset_size = int(0.1 * len(dataset))  # 10% of dataset\n",
    "subset_indices = random.sample(range(len(dataset)), subset_size)  # Random sampling\n",
    "subset_dataset = Subset(dataset, subset_indices)\n",
    "\n",
    "# DataLoader\n",
    "dataloader = DataLoader(subset_dataset, batch_size=40, shuffle=True)\n",
    "example_batch, example_labels = next(iter(dataloader))\n",
    "example_batch = example_batch.to(device)\n",
    "example_labels = example_labels.to(device)\n",
    "\n",
    "# Limit to 10 samples for inversion demo\n",
    "example_batch = example_batch[:10]\n",
    "example_labels = example_labels[:10] % NUM_CLASSES\n",
    "\n",
    "# Model definition\n",
    "class FaceRecognitionCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FaceRecognitionCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 36, kernel_size=7)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(36 * (IMG_X_SIZE - 6) * (IMG_Y_SIZE - 6), NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "model = FaceRecognitionCNN().to(device)\n",
    "\n",
    "# Training setup\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Early stopping setup\n",
    "best_loss = float('inf')\n",
    "best_accuracy = 0.0\n",
    "patience = 5\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# Training loop with early stopping\n",
    "EPOCHS = 20\n",
    "print(\"Training model...\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    count = 0\n",
    "\n",
    "    for X, y in dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y = y % NUM_CLASSES\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = output.argmax(dim=1)\n",
    "        acc = (preds == y).float().mean().item()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc += acc\n",
    "        count += 1\n",
    "\n",
    "    avg_loss = total_loss / count\n",
    "    avg_acc = total_acc / count\n",
    "    print(f\"Epoch {epoch}/{EPOCHS} — Loss: {avg_loss:.4f} — Accuracy: {avg_acc:.4f}\")\n",
    "\n",
    "    if avg_loss < best_loss and avg_acc > best_accuracy:\n",
    "        best_loss = avg_loss\n",
    "        best_accuracy = avg_acc\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch} epochs.\")\n",
    "            break\n",
    "\n",
    "# Save model checkpoint\n",
    "print(\"Saving model checkpoint...\")\n",
    "torch.save(model.state_dict(), MODEL_CHECKPOINT_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75cedfa",
   "metadata": {},
   "source": [
    "# Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe499020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Constants\n",
    "IMG_X_SIZE = 218\n",
    "IMG_Y_SIZE = 178\n",
    "NUM_CLASSES = 8192\n",
    "MODEL_CHECKPOINT_PATH = './face_recognition_model.pth'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Model definition\n",
    "class FaceRecognitionCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FaceRecognitionCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 36, kernel_size=7)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(36 * (IMG_X_SIZE - 6) * (IMG_Y_SIZE - 6), NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "# Load model\n",
    "model = FaceRecognitionCNN().to(device)\n",
    "model.load_state_dict(torch.load(MODEL_CHECKPOINT_PATH))\n",
    "model.eval()\n",
    "\n",
    "# Inversion function (corrected leaf tensor handling)\n",
    "def sarahs_inversion(model, label_idx, steps=100, lr=0.01, noise_strength=0.001):\n",
    "    model.eval()\n",
    "    label = torch.tensor([label_idx], device=device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Create a leaf tensor with requires_grad=True\n",
    "    img_data = torch.randn((1, 1, IMG_X_SIZE, IMG_Y_SIZE), device=device) * 0.1\n",
    "    img_data = img_data.clamp(0, 1)\n",
    "    img_data.requires_grad_()\n",
    "\n",
    "    for step in range(steps):\n",
    "        if img_data.grad is not None:\n",
    "            img_data.grad.zero_()\n",
    "\n",
    "        output = model(img_data)\n",
    "        loss = loss_fn(output, label)\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            img_data -= lr * img_data.grad\n",
    "            # img_data += torch.randn_like(img_data) * noise_strength\n",
    "            img_data.clamp_(0, 1)\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print(f\"Step {step}/{steps} — Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return img_data.detach()\n",
    "\n",
    "# Side-by-side plot\n",
    "def plot_comparison(original, reconstructed, index):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(6, 3))\n",
    "    axs[0].imshow(original.squeeze().cpu(), cmap='gray')\n",
    "    axs[0].set_title(\"Original\")\n",
    "    axs[0].axis(\"off\")\n",
    "\n",
    "    axs[1].imshow(reconstructed.squeeze().cpu(), cmap='gray')\n",
    "    axs[1].set_title(\"Reconstructed\")\n",
    "    axs[1].axis(\"off\")\n",
    "\n",
    "    plt.suptitle(f\"Example {index}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Reconstruction\n",
    "print(\"Reconstructing images...\")\n",
    "for idx in range(10):\n",
    "    label = example_labels[idx].item()\n",
    "    target_img = example_batch[idx:idx+1]\n",
    "    recon_img = sarahs_inversion(model, label_idx=label, steps=100, lr=0.01, noise_strength=0.001)\n",
    "    plot_comparison(target_img, recon_img, idx)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
